{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5.4 - Homework 2 - Setup.ipynb","provenance":[{"file_id":"1BAmBGJbmjie6j5ZsyVx9771udKCSgh75","timestamp":1644707049353}],"collapsed_sections":[],"authorship_tag":"ABX9TyPpfklqSJeao4l6pIQhbNr5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**HOMEWORK 2: REAL ESTATE PRICE PREDICTION (Total: / 25 points)**"],"metadata":{"id":"U8YOCpyn-rKa"}},{"cell_type":"markdown","source":["# *Names: (Student One, Student Two)*"],"metadata":{"id":"sMblm7NpWSY9"}},{"cell_type":"markdown","source":["Make sure that you answer the two conceptual questions provided at the front of the notebook (edit the XXX response sections to provide your answers). You probably want to answer these last, after you finish the hands-on portion of the assignment. "],"metadata":{"id":"r0tuEldE-wqV"}},{"cell_type":"markdown","source":["#*Background: Use Case*"],"metadata":{"id":"5D994NiULYme"}},{"cell_type":"markdown","source":["Your task here is to use the supplied dataset describing homes, including four photos per home and some other structured features, along with an associated home sale price label, to try to fit a predictive model of sale price.\n","\n","The use case here is as follows. Your customers are individual sellers, buyers, and real estate agent, who wishes to set their sale price, or their offer price on a new home. You can assume the user will always have these features available to them at the time of prediction (i.e., the four photo types in question, as well as the structured features). \n","\n","You must evaluate your model's performance using 5-fold cross validation. I will evaluate your model's performance (in terms of MAE) based on validation loss."],"metadata":{"id":"pNPy9KrF0Rbs"}},{"cell_type":"markdown","source":["#**Answers to Written Questions (2 points)**"],"metadata":{"id":"vMGozfk1r9CO"}},{"cell_type":"markdown","source":["**Question 1: Why is it necessaty to resize the images? What impact, conceptually, would the choice of image size have on the prediction task, computationally / practically, but also in terms of prediction performnace (note: I'm not asking you try out different sizes to see what happens; I'm asking you to think about 'why' a choice of a smaller vs. larger image resizing would matter.)**"],"metadata":{"id":"epbB9Es8srBx"}},{"cell_type":"markdown","source":["XXX"],"metadata":{"id":"3ZGbcpYis1S8"}},{"cell_type":"markdown","source":["**Question 2: Are there any predictors that you decided to discard here (numeric or image-based)? Why or why not; what did you consider when making these choices? Did you add any features? What, and why?**"],"metadata":{"id":"Hw75Duf_eQqE"}},{"cell_type":"markdown","source":["XXX"],"metadata":{"id":"J8O_zKBYf2Wj"}},{"cell_type":"markdown","source":["#**Importing the Data**"],"metadata":{"id":"BeNdB8Y7tNLM"}},{"cell_type":"markdown","source":["I've provided some starter code for importing the data..."],"metadata":{"id":"RwIETaIYmObU"}},{"cell_type":"code","source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","import glob\n","from PIL import Image\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Replace with your basepath to the extracted dataset in your Google Drive\n","basePath = 'drive/MyDrive/XXX'\n","\n","# These are the column headers for the home characteristics data.\n","cols = ['bedrooms','bathrooms','area','zipcode','price']\n","\n","# We first pull in the numeric features for houses from the text file.\n","numericData = pd.read_csv(f'{basePath}/all/HousesInfo.txt', sep=\" \", header=None, names=cols)\n","\n","# Pulling out the labels\n","labels = numericData['price']\n","\n","image_size = (200,200)\n","\n","# Reading the images into memory may take a minute - be patient :).\n","def read_images(imageType):\n","  image_list = []\n","  for file in glob.glob(f\"{basePath}/all/{imageType}/*\"):\n","    im=Image.open(file)\n","    im=im.resize(size=image_size)\n","    image_list.append(np.divide(np.array(im),255))\n","  return np.stack(image_list)\n","\n","front_image = read_images(\"frontal\")\n","bathroom_image = read_images(\"bathroom\")\n","bedroom_image = read_images(\"bedroom\")\n","kitchen_image = read_images(\"kitchen\")\n","\n","print(f'The shape of our kitchen image array is {kitchen_image.shape}')\n","\n","# Our 535 houses' images are all now in memory as 200x200 pictures, with 3 channels (RGB). Feel free to change the image size.\n","for i in range(3):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(kitchen_image[i])\n","    plt.axis(\"off\")"],"metadata":{"id":"a2ab2j0wz3oA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Create a Multi-Modal NN from Scratch (13 points)**"],"metadata":{"id":"vhnwnEtEOeXd"}},{"cell_type":"markdown","source":["#*Define Your Model*"],"metadata":{"id":"A6ihDDUOLJo2"}},{"cell_type":"markdown","source":["Provide your code to implement a multi-modal NN. You should evaluate your model's loss in terms of mean absolute error (MAE)."],"metadata":{"id":"RGYaiNdpoako"}},{"cell_type":"code","source":["def build_model():\n","    \n","    # PROVIDE YOUR CODE HERE\n","\n","    model.compile(loss=\"mae\")\n","\n","    return model\n","\n","#model.summary()\n","#keras.utils.plot_model(model, show_shapes=True)"],"metadata":{"id":"8JBFs2OXdZzU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#*Fit Your Model*"],"metadata":{"id":"n5JOrAqTLF-O"}},{"cell_type":"markdown","source":["Train your model and track validation loss in terms of MAE. You must use 5-fold cross-validation here. I will evaluate your model's performance in terms of best average validation loss."],"metadata":{"id":"fAdJc4v1Bx4d"}},{"cell_type":"code","source":["### PROVIDE YOUR CODE TO EVALUATE FIT / EVALUATE YOUR MODEL'S ACCURACY"],"metadata":{"id":"NLZ3t7D9l31w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot your model's loss over epochs to undertand fitting / overfitting."],"metadata":{"id":"XcD7R5-UG8sl"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# YOUR CODE HERE"],"metadata":{"id":"jXQEVK1bHAHy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Now Use a Pre-Trained Model (10 points)**"],"metadata":{"id":"3X1CKkcYOe97"}},{"cell_type":"markdown","source":["In this section, you should replace the CNN components of your 'from-scratch' multi-modal network with a pre-trained model. You can use any pre-trained model you like to pre-process your images into numeric vectors, and then feed those vectors into a standard, dense feed-forward NN to generate predictions. "],"metadata":{"id":"gXW7KlegOk7J"}},{"cell_type":"markdown","source":["#*Load Pre-Trained Model's CNN layers*"],"metadata":{"id":"5g9_-EMaCkwS"}},{"cell_type":"code","source":["# YOUR CODE HERE TO LOAD THE PRE-TRAINED MODEL"],"metadata":{"id":"WNcddw2hO35M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#*Pre-process Your Images Through the Pre-trained Model's CNN Layers*"],"metadata":{"id":"gajJd26YnnAS"}},{"cell_type":"markdown","source":["You need to write a quick function that loops over your images and runs them through the pre-trained CNN layers to get back the numeric vector representations."],"metadata":{"id":"RF6MTnrkG-5w"}},{"cell_type":"code","source":["## YOUR CODE HERE TO PROCESS IMAGES THROUGH THE PRE-TRAINED MODEL AND STORE THEIR FEATURE VECTORS\n","\n","# Recall we already created labels and homeFeatures data earlier, which are still numpy arrays in memory.\n","print(homeFeatures.shape)\n","print(labels.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdbXu3c-C-0a","executionInfo":{"status":"ok","timestamp":1644705092914,"user_tz":300,"elapsed":19942,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}},"outputId":"fc4ea318-bc77-4624-8f67-f4034218d734"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(535, 7, 7, 2048)\n","(535, 7)\n","(535,)\n"]}]},{"cell_type":"markdown","source":["#*Define Your Dense Network (That Accepts the Image's Feature Vectors)*"],"metadata":{"id":"7kVvPgayE3On"}},{"cell_type":"markdown","source":["You will once again have a multi-branch network, but you've no longer any need for CNNs here. Just use Dense layers for each branch of input, and merge the branches together before producing a numeric prediction. Of course you may want to add elements to address overfitting."],"metadata":{"id":"qI35k0K6E83T"}},{"cell_type":"code","source":["def build_model_preTrain():\n","\n","    ### YOUR CODE HERE TO BUILD A DENSE NETWORK ON THE FLY\n","\n","    model_preTrain.compile(loss=\"mae\", optimizer=\"adam\")\n","\n","    return model_preTrain"],"metadata":{"id":"RtEr2ADWE7-v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#*Fit Your Dense Model*"],"metadata":{"id":"-J-ivLplKdF-"}},{"cell_type":"markdown","source":["Again, make sure you employ 5-fold cross validation, and employ MAE as your loss function."],"metadata":{"id":"nbYd4SynCoSm"}},{"cell_type":"code","source":["### YOUR CODE HERE TO EVALUATE ACCURACY LEVERAGING THE PRE-TRAINED MODEL"],"metadata":{"id":"mjAA4QQzKdU0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, plot your model's validation loss over training. "],"metadata":{"id":"CRSYQc9THy9w"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Fill this in."],"metadata":{"id":"AYxKuzXNIdFo"},"execution_count":null,"outputs":[]}]}