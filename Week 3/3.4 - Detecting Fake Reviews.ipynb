{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.4 - Detecting Fake Reviews.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN1ae29FAwhW8a8UAeXD/1x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**Detecting Fake Reviews**"],"metadata":{"id":"iqBsw2Dyo1BA"}},{"cell_type":"markdown","source":["This dataset is the product of research by a few folks in Computer Science: https://aclanthology.org/N13-1053.pdf. There is even a consumer-facing tool based on the model, here: http://reviewskeptic.com/. "],"metadata":{"id":"KIEFBE0apGc3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7R5wj0jhcY1"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from google.colab import files\n","import pandas as pd\n","import io\n","\n","trip_advisor = pd.read_csv('https://raw.githubusercontent.com/gburtch/BA865-2022/main/Week%203/datasets/deceptive-opinion.csv')\n","\n","#uploaded = files.upload()\n","#trip_advisor = pd.read_csv(io.BytesIO(uploaded['deceptive-opinion.csv']))\n","trip_advisor.describe(include='all')"]},{"cell_type":"markdown","source":["We can use some of the Keras utilities to pre-process the text. \n","\n","*Q: What features should we use for our prediction?*"],"metadata":{"id":"uFvjAtrpoxb-"}},{"cell_type":"code","source":["import numpy as np\n","from keras.preprocessing.text import text_to_word_sequence\n","\n","# The dataset is perfectly balanced, so 50% accuracy will be equivalent to a random guess. \n","labels = np.where(trip_advisor['deceptive']=='truthful',0,1)\n","\n","text = []\n","for i in range(len(trip_advisor)):\n","  text.append(text_to_word_sequence(trip_advisor['text'][i])) # This strips punctuation, odd characters, and makes things lower-case. "],"metadata":{"id":"_8dsXMrbk445"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is another pre-processing step, but it's optional. \n","\n","*Q: What does this code block do?*"],"metadata":{"id":"PFNRsZqu3JNl"}},{"cell_type":"code","source":["min_freq = 1\n","\n","word_freq = {}\n","for review in text:\n","  for term in review:\n","    try:\n","        word_freq[term] = word_freq[term]+1\n","    except KeyError:\n","        word_freq[term] = 1\n","\n","max_freq = max(i for i in word_freq.values())\n","for i in range(len(text)):\n","  text[i] = [term for term in text[i] if word_freq[term] >= min_freq & word_freq[term] <= max_freq]"],"metadata":{"id":"kySkp5Gi2_yv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, we are making our integer codings for the text tokens."],"metadata":{"id":"E4bJC8IWkkKa"}},{"cell_type":"code","source":["# We declare a set, which we populate from terms from the corpus, one by one. \n","# Sets only allow 'unique' values. \n","unique_terms = {term for review in text for term in review}\n","print(f'We have {len(unique_terms)} unique tokens in our dataset.')\n","\n","# We can then easily make a term-integer dictionary and an integer-term dictionary (for reverse lookup)\n","word_index = {term: number for number, term in enumerate(unique_terms)}\n","reverse_index = {number: term for number, term in enumerate(unique_terms)}"],"metadata":{"id":"L4XcXwCDki8A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One-hot encoding the text can be done very explicitly now, as a nested loop."],"metadata":{"id":"mvIDdBCaiY1f"}},{"cell_type":"code","source":["def vectorize_sequences(sequences, dimension=len(unique_terms)): \n","    # Make our blank matrix of 0's to store hot encodings.\n","    results = np.zeros((len(sequences), dimension))\n","\n","    # For each observation and element in that observation,\n","    # Update the blank matrix to a 1 at row obs, column element value.\n","    for i, sequence in enumerate(sequences):\n","        for j in sequence:\n","            results[i, word_index[j]] = 1.\n","    return results\n","\n","text_onehot = vectorize_sequences(text)\n","\n","text_onehot.shape"],"metadata":{"id":"NdGsdLNxqyyf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Don't forget we have a few other features in the data. It's not just about the review text... "],"metadata":{"id":"4MONCGalmvsS"}},{"cell_type":"code","source":["# One hot encoding the hotels. \n","hotel_dict = {hotel : index for index, hotel in enumerate(set(trip_advisor['hotel']))}\n","hotels = []\n","for hotel in trip_advisor['hotel']:\n","  hotels.append(hotel_dict[hotel])\n","\n","hotels_onehot = keras.utils.to_categorical(np.array(hotels))\n","\n","# One hot encoding the review source\n","source_int = np.where(np.array(trip_advisor['source'])=='MTurk',0,np.where(np.array(trip_advisor['source'])=='TripAdvisor',1,2))\n","source_onehot = keras.utils.to_categorical(source_int)\n","\n","# Binarizing the polarity\n","polarity_bin = np.where(np.array(trip_advisor['polarity'])==\"negative\",0,1).reshape(1600,1)\n","\n","# Last step, we shuffle the data\n","data_onehot = np.concatenate((labels.reshape(1600,1),text_onehot,hotels_onehot,polarity_bin),axis=1)\n","np.random.shuffle(data_onehot)\n","\n","# Then we pull out predictors and labels.\n","predictors = data_onehot[:,1:]\n","labels = data_onehot[:,0]\n"],"metadata":{"id":"Jr3WnmXhmzcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can fit out model to the resulting data. Once again we have k-fold cross validation here. This model is incapable of learning anything much (validation accuracy never really surpasses 55-56%, and the loss is always increasing in training)."],"metadata":{"id":"pR9UI8U4zkjm"}},{"cell_type":"code","source":["from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","\n","def build_model():\n","    model = keras.Sequential([\n","        # This is essentially a dense layer that mimics word embedding; we are reducing the one-hot encoded text (10,000+ one-hot tokens) down to 750 latent dimensions.\n","        layers.Dense(750, activation=\"linear\"),\n","        layers.Dense(50, activation=\"relu\",kernel_regularizer=\"l2\"),\n","        layers.Dense(5, activation=\"relu\"),\n","        layers.Dense(1, activation=\"sigmoid\")\n","    ])\n","    model.compile(optimizer=keras.optimizers.Adadelta(learning_rate=0.01), loss=\"binary_crossentropy\", metrics=[keras.metrics.BinaryAccuracy(threshold=0.5)])\n","    return model\n","\n","model = build_model()\n","\n","data_train = predictors[:1200]\n","labels_train = labels[:1200]\n","data_test = predictors[1200:]\n","labels_test = labels[1200:]\n","\n","k = 4\n","num_validation_samples = len(data_train) // k\n","num_epochs = 50\n","batch_sizes = 25\n","all_loss_histories = []\n","all_val_loss_histories = []  \n","all_acc_histories = []\n","all_val_acc_histories = []\n","\n","# For each validation fold, we will train a full set of epochs, and store the history. \n","for fold in range(k):\n","    validation_data = data_train[num_validation_samples * fold:\n","                           num_validation_samples * (fold + 1)]\n","    validation_targets = labels_train[num_validation_samples * fold:\n","                           num_validation_samples * (fold + 1)]\n","    training_data = np.concatenate([\n","        data_train[:num_validation_samples * fold],\n","        data_train[num_validation_samples * (fold + 1):]])\n","    training_targets = np.concatenate([\n","        labels_train[:num_validation_samples * fold],\n","        labels_train[num_validation_samples * (fold + 1):]])\n","    model = build_model()\n","    history = model.fit(training_data, training_targets, \n","                        validation_data = (validation_data,validation_targets), \n","                        epochs=num_epochs, batch_size=batch_sizes)\n","    val_loss_history = history.history['val_loss']\n","    val_acc_history = history.history['val_binary_accuracy']\n","    loss_history = history.history['loss']\n","    acc_history = history.history['binary_accuracy']\n","    all_val_loss_histories.append(val_loss_history)\n","    all_loss_histories.append(loss_history)\n","    all_val_acc_histories.append(val_acc_history)\n","    all_acc_histories.append(acc_history)"],"metadata":{"id":"8sEQSBHNiGSo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Q: What does 'x' represent in these iterators?*\n","\n","*Q: Can anyone explain in simple words what this code is doing?*"],"metadata":{"id":"ERQl3xUL2K1Y"}},{"cell_type":"code","source":["average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n","average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n","average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n","average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]"],"metadata":{"id":"OIdUNbd02LoL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we plot the cross-validated performance:"],"metadata":{"id":"Ba1kaeAd2gax"}},{"cell_type":"code","source":["# Plot validation performance. \n","plt.plot(average_loss_history,c='r')\n","plt.plot(average_acc_history,c=\"r\",linestyle=\"dashed\")\n","plt.plot(average_val_loss_history,c='b')\n","plt.plot(average_val_acc_history,c='b',linestyle=\"dashed\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend(['Training Loss','Training Accuracy','Validation Loss','Validation Accuracy'])\n","plt.show()"],"metadata":{"id":"rRmEzDzHVeh9"},"execution_count":null,"outputs":[]}]}