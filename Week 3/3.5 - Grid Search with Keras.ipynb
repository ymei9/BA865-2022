{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.5 - Grid Search with Keras.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPqjP5hcH4zQqGoHfKxpPeb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kEUXk5xlvg9e"},"outputs":[],"source":["try:\n","    from scikeras.wrappers import KerasRegressor                     \n","except ImportError:\n","    !pip install scikeras\n","    from scikeras.wrappers import KerasRegressor\n","    \n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import sklearn\n","from sklearn.pipeline import Pipeline # for setting up a pre-processing / tuning pipeline.\n","from sklearn.preprocessing import RobustScaler # Here, we are going to normalize inputs (the ML Pipeline framework from sklearn can implement this.)\n","\n","# So, we are going back to the Boston Housing data here.\n","from tensorflow.keras.datasets import boston_housing\n","(train_data, train_targets), (test_data, test_targets) = (boston_housing.load_data())"]},{"cell_type":"markdown","source":["#*Grid Search CV With Keras Model*"],"metadata":{"id":"Xvf2i7v1zv61"}},{"cell_type":"code","source":["# Make sure you set your custom parameters for training as arguments in your model creation function.\n","def create_model(optimizer=\"sgd\",activation=\"relu\",units=100,numLayers=2, batch_size=10):\n","    \n","    # I beleve that you need to explicitly declare an input layer for the scikeras wrapper to work... \n","    model = keras.Sequential([\n","        layers.Input(train_data.shape[1]),\n","        layers.Dense(units, activation=\"relu\")             \n","    ])\n","\n","    if numLayers == 2:\n","        model.add(layers.Dense(units, activation=\"relu\"))\n","\n","    model.add(layers.Dense(1, activation=activation))\n","\n","    model.compile(loss='mean_squared_error',optimizer=optimizer, metrics=['mse'])\n","    return model\n","\n","# You also need to specify the 'custom' parameters here that you want to add, for them to show up as a trainable parameter in GridSearchCV.\n","regf = KerasRegressor(model=create_model, optimizer=\"adam\", activation=\"relu\", units=100, numLayers=2, batch_size=10, verbose=0)\n","\n","# Note you can also do a grid search over an sklearn pipeline, so you can search over diferent types of data pre-processing approaches too!\n","ml_pipeline = Pipeline([(\"Normalize_with_centering\", RobustScaler()), (\"Model\", regf)])\n","\n","# Here are the configurable parameters we can now search over for either object. \n","print(regf.get_params().keys())\n","print(ml_pipeline.get_params().keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dpJeCA_BvdCj","executionInfo":{"status":"ok","timestamp":1643679031749,"user_tz":300,"elapsed":295,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}},"outputId":"1af60014-ecab-43c7-f2df-c265575597e5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['model', 'build_fn', 'warm_start', 'random_state', 'optimizer', 'loss', 'metrics', 'batch_size', 'validation_batch_size', 'verbose', 'callbacks', 'validation_split', 'shuffle', 'run_eagerly', 'epochs', 'activation', 'units', 'numLayers'])\n","dict_keys(['memory', 'steps', 'verbose', 'Normalize_with_centering', 'Model', 'Normalize_with_centering__copy', 'Normalize_with_centering__quantile_range', 'Normalize_with_centering__unit_variance', 'Normalize_with_centering__with_centering', 'Normalize_with_centering__with_scaling', 'Model__model', 'Model__build_fn', 'Model__warm_start', 'Model__random_state', 'Model__optimizer', 'Model__loss', 'Model__metrics', 'Model__batch_size', 'Model__validation_batch_size', 'Model__verbose', 'Model__callbacks', 'Model__validation_split', 'Model__shuffle', 'Model__run_eagerly', 'Model__epochs', 'Model__activation', 'Model__units', 'Model__numLayers'])\n"]}]},{"cell_type":"markdown","source":["And this is how I would invoke my grid search... "],"metadata":{"id":"twEp-TV23rUM"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","import numpy as np\n","\n","# Because we are creating the models but are not compiling them yet (we will let the grid fit compile the models on the fly),\n","# this will produce a bunch of warnings. I'm just suppressing the warnings. \n","#import logging, os\n","#logging.disable(logging.WARNING)\n","#os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n","\n","params = {\n","    \"numLayers\": [1,2],\n","    \"units\": [100,125,150],\n","    \"activation\": ['relu','selu',None],\n","    \"batch_size\": [25,50],\n","    \"epochs\": [10,20,30]\n","}\n","\n","params_pipe = {\n","    \"Model__numLayers\": [1,2],\n","    \"Model__units\": [100,500],\n","    \"Model__activation\": ['relu','selu',None],\n","    \"Model__batch_size\": [25,50],\n","    \"Model__epochs\":[10,20,30]\n","}\n","\n","grid = GridSearchCV(regf, params, scoring='neg_mean_absolute_error',verbose=11)#,cv=10)\n","#grid = GridSearchCV(ml_pipeline, params_pipe, scoring='neg_mean_absolute_error',verbose=11)#,cv=10)\n","\n","grid.fit(train_data, train_targets)"],"metadata":{"id":"CLhnTa921I0-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I can then extract the parameters that yielded the top performance... "],"metadata":{"id":"L-ZBoMDM3wKF"}},{"cell_type":"code","source":["print(f\"Best Score  : {grid.best_score_}\")\n","print(f\"Best Params : {grid.best_params_}\")"],"metadata":{"id":"UISkU-jewLVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, a little function that looks at pairs of parameter values, and the associated model performance, holding all other parameters to their ideal values. "],"metadata":{"id":"F8X2ajBn32fa"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","def plot_results(index='units', columns='activation'):\n","    index = 'param_' + index\n","    columns = 'param_' + columns\n","\n","    # prepare the results into a pandas.DataFrame\n","    df = pd.DataFrame(grid.cv_results_)\n","\n","    # Remove the other by selecting their best values (from gscv.best_params_)\n","    other = [c for c in df.columns if c[:6] == 'param_']\n","    other.remove(index)\n","    other.remove(columns)\n","\n","    # Set all other parameters to their \"top\" values.\n","    for col in other:\n","        df = df[df[col] == grid.best_params_[col[6:]]]\n","\n","    # Create pivot tables for easy plotting\n","    table_mean = df.pivot_table(index=index, columns=columns,\n","                                values=['mean_test_score'])\n","    \n","    # plot the pivot tables\n","    plt.figure()\n","    ax = plt.gca()\n","    for col_mean in table_mean.columns:\n","        table_mean[col_mean].plot(marker='o',label=col_mean)\n","    plt.title('Grid-search results (higher is better)')\n","    plt.ylabel('- Mean Absolute Error')\n","    plt.legend(title=table_mean.columns.names)\n","    plt.show()\n","\n","plot_results(index='units', columns='activation')"],"metadata":{"id":"eRilfjohwhxn"},"execution_count":null,"outputs":[]}]}